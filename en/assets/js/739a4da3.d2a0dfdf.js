"use strict";(self.webpackChunkpromptgineering=self.webpackChunkpromptgineering||[]).push([[170],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>d});var n=t(67294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=n.createContext({}),f=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},p=function(e){var a=f(e.components);return n.createElement(l.Provider,{value:a},e.children)},m="mdxType",g={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},h=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=f(t),h=r,d=m["".concat(l,".").concat(h)]||m[h]||g[h]||o;return t?n.createElement(d,i(i({ref:a},p),{},{components:t})):n.createElement(d,i({ref:a},p))}));function d(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=h;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s[m]="string"==typeof e?e:r,i[1]=s;for(var f=2;f<o;f++)i[f]=t[f];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}h.displayName="MDXCreateElement"},98363:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>i,default:()=>g,frontMatter:()=>o,metadata:()=>s,toc:()=>f});var n=t(87462),r=(t(67294),t(3905));const o={sidebar_position:1e3},i="\ud83d\udcda Bibliography",s={unversionedId:"bibliography",id:"bibliography",title:"\ud83d\udcda Bibliography",description:"The page contains an organized list of all papers used by this course.",source:"@site/docs/bibliography.md",sourceDirName:".",slug:"/bibliography",permalink:"/en/docs/bibliography",draft:!1,editUrl:"https://github.com/trigaten/promptgineering/tree/v1.2.3/docs/bibliography.md",tags:[],version:"current",sidebarPosition:1e3,frontMatter:{sidebar_position:1e3},sidebar:"tutorialSidebar",previous:{title:"\ud83d\udcd9 Vocabulary Reference",permalink:"/en/docs/vocabulary"},next:{title:"\ud83d\udce6 Prompted Products",permalink:"/en/docs/products"}},l={},f=[{value:"Prompt Engineering Strategies",id:"prompt-engineering-strategies",level:2},{value:"Chain of Thought(@wei2022chain) \ud83d\udd35",id:"chain-of-thoughtwei2022chain-",level:4},{value:"Zero Shot Chain of Thought(@kojima2022large) \ud83d\udd35",id:"zero-shot-chain-of-thoughtkojima2022large-",level:4},{value:"Self Consistency(@wang2022selfconsistency) \ud83d\udd35",id:"self-consistencywang2022selfconsistency-",level:4},{value:"What Makes Good In-Context Examples for GPT-3?(@liu2021makes) \ud83d\udd35",id:"what-makes-good-in-context-examples-for-gpt-3liu2021makes-",level:4},{value:"Ask-Me-Anything Prompting(@arora2022ama) \ud83d\udd35",id:"ask-me-anything-promptingarora2022ama-",level:3},{value:"Generated Knowledge(@liu2021generated) \ud83d\udd35",id:"generated-knowledgeliu2021generated-",level:4},{value:"Recitation-Augmented Language Models(@sun2022recitationaugmented) \ud83d\udd35",id:"recitation-augmented-language-modelssun2022recitationaugmented-",level:4},{value:"Rethinking the role of demonstrations(@min2022rethinking) \ud83d\udd35",id:"rethinking-the-role-of-demonstrationsmin2022rethinking-",level:4},{value:"Scratchpads(@nye2021work)",id:"scratchpadsnye2021work",level:4},{value:"Maieutic Prompting(@jung2022maieutic)",id:"maieutic-promptingjung2022maieutic",level:4},{value:"STaR(@zelikman2022star)",id:"starzelikman2022star",level:4},{value:"Least to Most(@zhou2022leasttomost) \ud83d\udd35",id:"least-to-mostzhou2022leasttomost-",level:4},{value:"Reframing Instructional Prompts to GPTk\u2019s Language(@mishra2022reframing) \ud83d\udd35",id:"reframing-instructional-prompts-to-gptks-languagemishra2022reframing-",level:4},{value:"The Turking Test: Can Language Models Understand Instructions?(@efrat2020turking) \ud83d\udd35",id:"the-turking-test-can-language-models-understand-instructionsefrat2020turking-",level:4},{value:"Reliability",id:"reliability",level:2},{value:"MathPrompter(@imani2023mathprompter) \ud83d\udd35",id:"mathprompterimani2023mathprompter-",level:4},{value:"The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning(@ye2022unreliability) \ud83d\udd35",id:"the-unreliability-of-explanations-in-few-shot-prompting-for-textual-reasoningye2022unreliability-",level:4},{value:"Prompting GPT-3 to be reliable(@si2022prompting)",id:"prompting-gpt-3-to-be-reliablesi2022prompting",level:4},{value:"Diverse Prompts(@li2022advance) \ud83d\udd35",id:"diverse-promptsli2022advance-",level:4},{value:"Calibrate Before Use: Improving Few-Shot Performance of Language Models(@zhao2021calibrate) \ud83d\udd35",id:"calibrate-before-use-improving-few-shot-performance-of-language-modelszhao2021calibrate-",level:4},{value:"Enhanced Self Consistency(@mitchell2022enhancing)",id:"enhanced-self-consistencymitchell2022enhancing",level:4},{value:"Bias and Toxicity in Zero-Shot CoT(@shaikh2022second) \ud83d\udd35",id:"bias-and-toxicity-in-zero-shot-cotshaikh2022second-",level:4},{value:"Constitutional AI: Harmlessness from AI Feedback (@bai2022constitutional) \ud83d\udd35",id:"constitutional-ai-harmlessness-from-ai-feedback-bai2022constitutional-",level:4},{value:"Compositional Generalization - SCAN(@lake2018scan)",id:"compositional-generalization---scanlake2018scan",level:4},{value:"Automated Prompt Engineering",id:"automated-prompt-engineering",level:2},{value:"AutoPrompt(@shin2020autoprompt) \ud83d\udd35",id:"autopromptshin2020autoprompt-",level:4},{value:"Automatic Prompt Engineer(@zhou2022large)",id:"automatic-prompt-engineerzhou2022large",level:4},{value:"Models",id:"models",level:2},{value:"Language Models",id:"language-models",level:3},{value:"GPT-3(@brown2020language) \ud83d\udd35",id:"gpt-3brown2020language-",level:4},{value:"GPT-3 Instruct(@ouyang2022training) \ud83d\udd35",id:"gpt-3-instructouyang2022training-",level:4},{value:"PaLM(@chowdhery2022palm) \ud83d\udd35",id:"palmchowdhery2022palm-",level:4},{value:"BLOOM(@scao2022bloom) \ud83d\udd35",id:"bloomscao2022bloom-",level:4},{value:"BLOOM+1 (more languages/ 0 shot improvements)(@yong2022bloom1)",id:"bloom1-more-languages-0-shot-improvementsyong2022bloom1",level:4},{value:"Jurassic 1(@lieberjurassic) \ud83d\udd35",id:"jurassic-1lieberjurassic-",level:4},{value:"GPT-J-6B(@wange2021gptj)",id:"gpt-j-6bwange2021gptj",level:4},{value:"Roberta(@liu2019roberta)",id:"robertaliu2019roberta",level:4},{value:"Image Models",id:"image-models",level:3},{value:"Stable Diffusion(@rombach2021highresolution) \ud83d\udd35",id:"stable-diffusionrombach2021highresolution-",level:4},{value:"DALLE(@ramesh2022hierarchical) \ud83d\udd35",id:"dalleramesh2022hierarchical-",level:4},{value:"Soft Prompting",id:"soft-prompting",level:2},{value:"Soft Prompting(@lester2021power) \ud83d\udd35",id:"soft-promptinglester2021power-",level:4},{value:"Interpretable Discretized Soft Prompts(@khashabi2021prompt) \ud83d\udd35",id:"interpretable-discretized-soft-promptskhashabi2021prompt-",level:4},{value:"Datasets",id:"datasets",level:2},{value:"MultiArith(@roy-roth-2015-solving) \ud83d\udd35",id:"multiarithroy-roth-2015-solving-",level:4},{value:"GSM8K(@cobbe2021training) \ud83d\udd35",id:"gsm8kcobbe2021training-",level:4},{value:"HotPotQA(@yang2018hotpotqa) \ud83d\udd35",id:"hotpotqayang2018hotpotqa-",level:4},{value:"Fever(@thorne2018fever) \ud83d\udd35",id:"feverthorne2018fever-",level:4},{value:"BBQ: A Hand-Built Bias Benchmark for Question Answering(@parrish2021bbq) \ud83d\udd35",id:"bbq-a-hand-built-bias-benchmark-for-question-answeringparrish2021bbq-",level:4},{value:"Image Prompt Engineering",id:"image-prompt-engineering",level:2},{value:"Taxonomy of prompt modifiers(@oppenlaender2022taxonomy)",id:"taxonomy-of-prompt-modifiersoppenlaender2022taxonomy",level:4},{value:"DiffusionDB(@wang2022diffusiondb)",id:"diffusiondbwang2022diffusiondb",level:4},{value:"The DALLE 2 Prompt Book(@parsons2022dalleprompt) \ud83d\udd35",id:"the-dalle-2-prompt-bookparsons2022dalleprompt-",level:4},{value:"Prompt Engineering for Text-Based Generative Art(@oppenlaender2022prompt) \ud83d\udd35",id:"prompt-engineering-for-text-based-generative-artoppenlaender2022prompt-",level:4},{value:"With the right prompt, Stable Diffusion 2.0 can do hands.(@blake2022with) \ud83d\udd35",id:"with-the-right-prompt-stable-diffusion-20-can-do-handsblake2022with-",level:4},{value:"Optimizing Prompts for Text-to-Image Generation(@hao2022optimizing)",id:"optimizing-prompts-for-text-to-image-generationhao2022optimizing",level:4},{value:"Prompt Engineering IDEs",id:"prompt-engineering-ides",level:2},{value:"Prompt IDE(@strobelt2022promptide) \ud83d\udd35",id:"prompt-idestrobelt2022promptide-",level:4},{value:"Prompt Source(@bach2022promptsource) \ud83d\udd35",id:"prompt-sourcebach2022promptsource-",level:4},{value:"PromptChainer(@wu2022promptchainer) \ud83d\udd35",id:"promptchainerwu2022promptchainer-",level:4},{value:"PromptMaker(@jiang2022promptmaker) \ud83d\udd35",id:"promptmakerjiang2022promptmaker-",level:4},{value:"Tooling",id:"tooling",level:2},{value:"LangChain(@Chase_LangChain_2022) \ud83d\udd35",id:"langchainchase_langchain_2022-",level:4},{value:"TextBox 2.0: A Text Generation Library with Pre-trained Language Models(@tang2022textbox) \ud83d\udd35",id:"textbox-20-a-text-generation-library-with-pre-trained-language-modelstang2022textbox-",level:4},{value:"OpenPrompt: An Open-source Framework for Prompt-learning(@ding2021openprompt) \ud83d\udd35",id:"openprompt-an-open-source-framework-for-prompt-learningding2021openprompt-",level:4},{value:"GPT Index(@Liu_GPT_Index_2022) \ud83d\udd35",id:"gpt-indexliu_gpt_index_2022-",level:4},{value:"Applied Prompt Engineering",id:"applied-prompt-engineering",level:2},{value:"Language Model Cascades(@dohan2022language)",id:"language-model-cascadesdohan2022language",level:4},{value:"MRKL(@karpas2022mrkl) \ud83d\udd35",id:"mrklkarpas2022mrkl-",level:4},{value:"ReAct(@yao2022react) \ud83d\udd35",id:"reactyao2022react-",level:4},{value:"PAL: Program-aided Language Models(@gao2022pal) \ud83d\udd35",id:"pal-program-aided-language-modelsgao2022pal-",level:4},{value:"User Interface Design",id:"user-interface-design",level:2},{value:"Design Guidelines for Prompt Engineering Text-to-Image Generative Models(@liu2022design)",id:"design-guidelines-for-prompt-engineering-text-to-image-generative-modelsliu2022design",level:4},{value:"Prompt Injection",id:"prompt-injection",level:2},{value:"Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods(@crothers2022machine) \ud83d\udd35",id:"machine-generated-text-a-comprehensive-survey-of-threat-models-and-detection-methodscrothers2022machine-",level:4},{value:"Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples(@branch2022evaluating) \ud83d\udd35",id:"evaluating-the-susceptibility-of-pre-trained-language-models-via-handcrafted-adversarial-examplesbranch2022evaluating-",level:4},{value:"Prompt injection attacks against GPT-3(@simon2022inject) \ud83d\udd35",id:"prompt-injection-attacks-against-gpt-3simon2022inject-",level:4},{value:"Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions(@goodside2022inject) \ud83d\udd35",id:"exploiting-gpt-3-prompts-with-malicious-inputs-that-order-the-model-to-ignore-its-previous-directionsgoodside2022inject-",level:4},{value:"adversarial-prompts(@chase2021adversarial) \ud83d\udd35",id:"adversarial-promptschase2021adversarial-",level:4},{value:"GPT-3 Prompt Injection Defenses(@goodside2021gpt) \ud83d\udd35",id:"gpt-3-prompt-injection-defensesgoodside2021gpt-",level:4},{value:"Talking to machines: prompt engineering &amp; injection(@christoph2022talking)",id:"talking-to-machines-prompt-engineering--injectionchristoph2022talking",level:4},{value:"Exploring Prompt Injection Attacks(@selvi2022exploring) \ud83d\udd35",id:"exploring-prompt-injection-attacksselvi2022exploring-",level:4},{value:"Using GPT-Eliezer against ChatGPT Jailbreaking(@armstrong2022using) \ud83d\udd35",id:"using-gpt-eliezer-against-chatgpt-jailbreakingarmstrong2022using-",level:4},{value:"Microsoft Bing Chat Prompt(@kevinbing)",id:"microsoft-bing-chat-promptkevinbing",level:4},{value:"Jailbreaking",id:"jailbreaking",level:2},{value:"Ignore Previous Prompt: Attack Techniques For Language Models(@perez2022jailbreak)",id:"ignore-previous-prompt-attack-techniques-for-language-modelsperez2022jailbreak",level:4},{value:"Lessons learned on Language Model Safety and misuse(@brundage_2022)",id:"lessons-learned-on-language-model-safety-and-misusebrundage_2022",level:4},{value:"Toxicity Detection with Generative Prompt-based Inference(@wang2022jailbreak)",id:"toxicity-detection-with-generative-prompt-based-inferencewang2022jailbreak",level:4},{value:"New and improved content moderation tooling(@markov_2022)",id:"new-and-improved-content-moderation-toolingmarkov_2022",level:4},{value:"OpenAI API(@openai_api) \ud83d\udd35",id:"openai-apiopenai_api-",level:4},{value:"OpenAI ChatGPT(@openai_chatgpt) \ud83d\udd35",id:"openai-chatgptopenai_chatgpt-",level:4},{value:"ChatGPT 4 Tweet(@alice2022jailbreak) \ud83d\udd35",id:"chatgpt-4-tweetalice2022jailbreak-",level:4},{value:"Acting Tweet(@miguel2022jailbreak) \ud83d\udd35",id:"acting-tweetmiguel2022jailbreak-",level:4},{value:"Research Tweet(@derek2022jailbreak) \ud83d\udd35",id:"research-tweetderek2022jailbreak-",level:4},{value:"Pretend Ability Tweet(@nero2022jailbreak) \ud83d\udd35",id:"pretend-ability-tweetnero2022jailbreak-",level:4},{value:"Responsibility Tweet(@nick2022jailbreak) \ud83d\udd35",id:"responsibility-tweetnick2022jailbreak-",level:4},{value:"Lynx Mode Tweet(@jonas2022jailbreak) \ud83d\udd35",id:"lynx-mode-tweetjonas2022jailbreak-",level:4},{value:"Sudo Mode Tweet(@sudo2022jailbreak) \ud83d\udd35",id:"sudo-mode-tweetsudo2022jailbreak-",level:4},{value:"Ignore Previous Prompt(@ignore_previous_prompt) \ud83d\udd35",id:"ignore-previous-promptignore_previous_prompt-",level:4},{value:"Updated Jailbreaking Prompts (@AI_jailbreak) \ud83d\udd35",id:"updated-jailbreaking-prompts-ai_jailbreak-",level:4},{value:"Surveys",id:"surveys",level:2},{value:"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing(@liu2021pretrain)",id:"pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processingliu2021pretrain",level:4},{value:"PromptPapers(@ning2022papers)",id:"promptpapersning2022papers",level:4},{value:"Dataset Generation",id:"dataset-generation",level:2},{value:"Discovering Language Model Behaviors with Model-Written Evaluations(@perez2022discovering)",id:"discovering-language-model-behaviors-with-model-written-evaluationsperez2022discovering",level:4},{value:"Selective Annotation Makes Language Models Better Few-Shot Learners(@su2022selective)",id:"selective-annotation-makes-language-models-better-few-shot-learnerssu2022selective",level:4},{value:"Applications",id:"applications",level:2},{value:"Atlas: Few-shot Learning with Retrieval Augmented Language Models(@izacard2022atlas)",id:"atlas-few-shot-learning-with-retrieval-augmented-language-modelsizacard2022atlas",level:4},{value:"STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension(@wang2022strudel)",id:"strudel-structured-dialogue-summarization-for-dialogue-comprehensionwang2022strudel",level:4},{value:"Miscl",id:"miscl",level:2},{value:"Prompting Is Programming: A Query Language For Large Language Models(@beurerkellner2022prompting)",id:"prompting-is-programming-a-query-language-for-large-language-modelsbeurerkellner2022prompting",level:4},{value:"Parallel Context Windows Improve In-Context Learning of Large Language Models(@ratner2022parallel)",id:"parallel-context-windows-improve-in-context-learning-of-large-language-modelsratner2022parallel",level:4},{value:"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT(@white2023prompt) \ud83d\udd35",id:"a-prompt-pattern-catalog-to-enhance-prompt-engineering-with-chatgptwhite2023prompt-",level:4},{value:"Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models(@bursztyn2022learning)",id:"learning-to-perform-complex-tasks-through-compositional-fine-tuning-of-language-modelsbursztyn2022learning",level:4},{value:"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks(@wang2022supernaturalinstructions)",id:"super-naturalinstructions-generalization-via-declarative-instructions-on-1600-nlp-taskswang2022supernaturalinstructions",level:4},{value:"Making Pre-trained Language Models Better Few-shot Learners(@gao2021making)",id:"making-pre-trained-language-models-better-few-shot-learnersgao2021making",level:4},{value:"Grounding with search results(@livin2022large)",id:"grounding-with-search-resultslivin2022large",level:4},{value:"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models(@dang2022prompt)",id:"how-to-prompt-opportunities-and-challenges-of-zero--and-few-shot-learning-for-human-ai-interaction-in-creative-applications-of-generative-modelsdang2022prompt",level:4},{value:"On Measuring Social Biases in Prompt-Based Multi-Task Learning(@akyrek2022measuring)",id:"on-measuring-social-biases-in-prompt-based-multi-task-learningakyrek2022measuring",level:4},{value:"Plot Writing From Pre-Trained Language Models(@jin2022plot) \ud83d\udd35",id:"plot-writing-from-pre-trained-language-modelsjin2022plot-",level:4},{value:"StereoSet: Measuring stereotypical bias in pretrained language models(@nadeem-etal-2021-stereoset)",id:"stereoset-measuring-stereotypical-bias-in-pretrained-language-modelsnadeem-etal-2021-stereoset",level:4},{value:"Survey of Hallucination in Natural Language Generation(@Ji_2022)",id:"survey-of-hallucination-in-natural-language-generationji_2022",level:4},{value:"Examples(@2022examples)",id:"examples2022examples",level:4},{value:"Wordcraft(@yuan2022wordcraft)",id:"wordcraftyuan2022wordcraft",level:4},{value:"PainPoints(@fadnavis2022pain)",id:"painpointsfadnavis2022pain",level:4},{value:"Self-Instruct: Aligning Language Model with Self Generated Instructions(@wang2022selfinstruct)",id:"self-instruct-aligning-language-model-with-self-generated-instructionswang2022selfinstruct",level:4},{value:"From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models(@guo2022images)",id:"from-images-to-textual-prompts-zero-shot-vqa-with-frozen-large-language-modelsguo2022images",level:4},{value:"Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference(@schick2020exploiting)",id:"exploiting-cloze-questions-for-few-shot-text-classification-and-natural-language-inferenceschick2020exploiting",level:4},{value:"Ask-Me-Anything Prompting(@arora2022ama)",id:"ask-me-anything-promptingarora2022ama",level:3},{value:"A Watermark for Large Language Models(@kirchenbauer2023watermarking)",id:"a-watermark-for-large-language-modelskirchenbauer2023watermarking",level:3}],p={toc:f},m="wrapper";function g(e){let{components:a,...t}=e;return(0,r.kt)(m,(0,n.Z)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"-bibliography"},"\ud83d\udcda Bibliography"),(0,r.kt)("p",null,"The page contains an organized list of all papers used by this course.\nThe papers are organized by topic."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"To cite this course, use the provided citation in the Github repository.")),(0,r.kt)("p",null,"\ud83d\udd35 = Paper directly cited in this course. Other papers have informed my understanding of the topic."),(0,r.kt)("p",null,"Note: since ",(0,r.kt)("a",{parentName:"p",href:"https://twitter.com/janleike/status/1584618242756132864"},"neither the GPT-3 nor the GPT-3 Instruct paper correspond to davinci models"),", I attempt not to\ncite them as such."),(0,r.kt)("h2",{id:"prompt-engineering-strategies"},"Prompt Engineering Strategies"),(0,r.kt)("h4",{id:"chain-of-thoughtwei2022chain-"},"Chain of Thought",(0,r.kt)("sup",{parentName:"h4",id:"fnref-1"},(0,r.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"zero-shot-chain-of-thoughtkojima2022large-"},"Zero Shot Chain of Thought",(0,r.kt)("sup",{parentName:"h4",id:"fnref-2"},(0,r.kt)("a",{parentName:"sup",href:"#fn-2",className:"footnote-ref"},"2"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"self-consistencywang2022selfconsistency-"},"Self Consistency",(0,r.kt)("sup",{parentName:"h4",id:"fnref-3"},(0,r.kt)("a",{parentName:"sup",href:"#fn-3",className:"footnote-ref"},"3"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"what-makes-good-in-context-examples-for-gpt-3liu2021makes-"},"What Makes Good In-Context Examples for GPT-3?",(0,r.kt)("sup",{parentName:"h4",id:"fnref-4"},(0,r.kt)("a",{parentName:"sup",href:"#fn-4",className:"footnote-ref"},"4"))," \ud83d\udd35"),(0,r.kt)("h3",{id:"ask-me-anything-promptingarora2022ama-"},"Ask-Me-Anything Prompting",(0,r.kt)("sup",{parentName:"h3",id:"fnref-5"},(0,r.kt)("a",{parentName:"sup",href:"#fn-5",className:"footnote-ref"},"5"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"generated-knowledgeliu2021generated-"},"Generated Knowledge",(0,r.kt)("sup",{parentName:"h4",id:"fnref-6"},(0,r.kt)("a",{parentName:"sup",href:"#fn-6",className:"footnote-ref"},"6"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"recitation-augmented-language-modelssun2022recitationaugmented-"},"Recitation-Augmented Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-7"},(0,r.kt)("a",{parentName:"sup",href:"#fn-7",className:"footnote-ref"},"7"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"rethinking-the-role-of-demonstrationsmin2022rethinking-"},"Rethinking the role of demonstrations",(0,r.kt)("sup",{parentName:"h4",id:"fnref-8"},(0,r.kt)("a",{parentName:"sup",href:"#fn-8",className:"footnote-ref"},"8"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"scratchpadsnye2021work"},"Scratchpads",(0,r.kt)("sup",{parentName:"h4",id:"fnref-9"},(0,r.kt)("a",{parentName:"sup",href:"#fn-9",className:"footnote-ref"},"9"))),(0,r.kt)("h4",{id:"maieutic-promptingjung2022maieutic"},"Maieutic Prompting",(0,r.kt)("sup",{parentName:"h4",id:"fnref-10"},(0,r.kt)("a",{parentName:"sup",href:"#fn-10",className:"footnote-ref"},"10"))),(0,r.kt)("h4",{id:"starzelikman2022star"},"STaR",(0,r.kt)("sup",{parentName:"h4",id:"fnref-11"},(0,r.kt)("a",{parentName:"sup",href:"#fn-11",className:"footnote-ref"},"11"))),(0,r.kt)("h4",{id:"least-to-mostzhou2022leasttomost-"},"Least to Most",(0,r.kt)("sup",{parentName:"h4",id:"fnref-12"},(0,r.kt)("a",{parentName:"sup",href:"#fn-12",className:"footnote-ref"},"12"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"reframing-instructional-prompts-to-gptks-languagemishra2022reframing-"},"Reframing Instructional Prompts to GPTk\u2019s Language",(0,r.kt)("sup",{parentName:"h4",id:"fnref-13"},(0,r.kt)("a",{parentName:"sup",href:"#fn-13",className:"footnote-ref"},"13"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"the-turking-test-can-language-models-understand-instructionsefrat2020turking-"},"The Turking Test: Can Language Models Understand Instructions?",(0,r.kt)("sup",{parentName:"h4",id:"fnref-14"},(0,r.kt)("a",{parentName:"sup",href:"#fn-14",className:"footnote-ref"},"14"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"reliability"},"Reliability"),(0,r.kt)("h4",{id:"mathprompterimani2023mathprompter-"},"MathPrompter",(0,r.kt)("sup",{parentName:"h4",id:"fnref-15"},(0,r.kt)("a",{parentName:"sup",href:"#fn-15",className:"footnote-ref"},"15"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"the-unreliability-of-explanations-in-few-shot-prompting-for-textual-reasoningye2022unreliability-"},"The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",(0,r.kt)("sup",{parentName:"h4",id:"fnref-16"},(0,r.kt)("a",{parentName:"sup",href:"#fn-16",className:"footnote-ref"},"16"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"prompting-gpt-3-to-be-reliablesi2022prompting"},"Prompting GPT-3 to be reliable",(0,r.kt)("sup",{parentName:"h4",id:"fnref-17"},(0,r.kt)("a",{parentName:"sup",href:"#fn-17",className:"footnote-ref"},"17"))),(0,r.kt)("h4",{id:"diverse-promptsli2022advance-"},"Diverse Prompts",(0,r.kt)("sup",{parentName:"h4",id:"fnref-18"},(0,r.kt)("a",{parentName:"sup",href:"#fn-18",className:"footnote-ref"},"18"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"calibrate-before-use-improving-few-shot-performance-of-language-modelszhao2021calibrate-"},"Calibrate Before Use: Improving Few-Shot Performance of Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-19"},(0,r.kt)("a",{parentName:"sup",href:"#fn-19",className:"footnote-ref"},"19"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"enhanced-self-consistencymitchell2022enhancing"},"Enhanced Self Consistency",(0,r.kt)("sup",{parentName:"h4",id:"fnref-20"},(0,r.kt)("a",{parentName:"sup",href:"#fn-20",className:"footnote-ref"},"20"))),(0,r.kt)("h4",{id:"bias-and-toxicity-in-zero-shot-cotshaikh2022second-"},"Bias and Toxicity in Zero-Shot CoT",(0,r.kt)("sup",{parentName:"h4",id:"fnref-21"},(0,r.kt)("a",{parentName:"sup",href:"#fn-21",className:"footnote-ref"},"21"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"constitutional-ai-harmlessness-from-ai-feedback-bai2022constitutional-"},"Constitutional AI: Harmlessness from AI Feedback",(0,r.kt)("sup",{parentName:"h4",id:"fnref-22"},(0,r.kt)("a",{parentName:"sup",href:"#fn-22",className:"footnote-ref"},"22"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"compositional-generalization---scanlake2018scan"},"Compositional Generalization - SCAN",(0,r.kt)("sup",{parentName:"h4",id:"fnref-23"},(0,r.kt)("a",{parentName:"sup",href:"#fn-23",className:"footnote-ref"},"23"))),(0,r.kt)("h2",{id:"automated-prompt-engineering"},"Automated Prompt Engineering"),(0,r.kt)("h4",{id:"autopromptshin2020autoprompt-"},"AutoPrompt",(0,r.kt)("sup",{parentName:"h4",id:"fnref-24"},(0,r.kt)("a",{parentName:"sup",href:"#fn-24",className:"footnote-ref"},"24"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"automatic-prompt-engineerzhou2022large"},"Automatic Prompt Engineer",(0,r.kt)("sup",{parentName:"h4",id:"fnref-25"},(0,r.kt)("a",{parentName:"sup",href:"#fn-25",className:"footnote-ref"},"25"))),(0,r.kt)("h2",{id:"models"},"Models"),(0,r.kt)("h3",{id:"language-models"},"Language Models"),(0,r.kt)("h4",{id:"gpt-3brown2020language-"},"GPT-3",(0,r.kt)("sup",{parentName:"h4",id:"fnref-26"},(0,r.kt)("a",{parentName:"sup",href:"#fn-26",className:"footnote-ref"},"26"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"gpt-3-instructouyang2022training-"},"GPT-3 Instruct",(0,r.kt)("sup",{parentName:"h4",id:"fnref-27"},(0,r.kt)("a",{parentName:"sup",href:"#fn-27",className:"footnote-ref"},"27"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"palmchowdhery2022palm-"},"PaLM",(0,r.kt)("sup",{parentName:"h4",id:"fnref-28"},(0,r.kt)("a",{parentName:"sup",href:"#fn-28",className:"footnote-ref"},"28"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"bloomscao2022bloom-"},"BLOOM",(0,r.kt)("sup",{parentName:"h4",id:"fnref-29"},(0,r.kt)("a",{parentName:"sup",href:"#fn-29",className:"footnote-ref"},"29"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"bloom1-more-languages-0-shot-improvementsyong2022bloom1"},"BLOOM+1 (more languages/ 0 shot improvements)",(0,r.kt)("sup",{parentName:"h4",id:"fnref-30"},(0,r.kt)("a",{parentName:"sup",href:"#fn-30",className:"footnote-ref"},"30"))),(0,r.kt)("h4",{id:"jurassic-1lieberjurassic-"},"Jurassic 1",(0,r.kt)("sup",{parentName:"h4",id:"fnref-31"},(0,r.kt)("a",{parentName:"sup",href:"#fn-31",className:"footnote-ref"},"31"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"gpt-j-6bwange2021gptj"},"GPT-J-6B",(0,r.kt)("sup",{parentName:"h4",id:"fnref-32"},(0,r.kt)("a",{parentName:"sup",href:"#fn-32",className:"footnote-ref"},"32"))),(0,r.kt)("h4",{id:"robertaliu2019roberta"},"Roberta",(0,r.kt)("sup",{parentName:"h4",id:"fnref-33"},(0,r.kt)("a",{parentName:"sup",href:"#fn-33",className:"footnote-ref"},"33"))),(0,r.kt)("h3",{id:"image-models"},"Image Models"),(0,r.kt)("h4",{id:"stable-diffusionrombach2021highresolution-"},"Stable Diffusion",(0,r.kt)("sup",{parentName:"h4",id:"fnref-34"},(0,r.kt)("a",{parentName:"sup",href:"#fn-34",className:"footnote-ref"},"34"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"dalleramesh2022hierarchical-"},"DALLE",(0,r.kt)("sup",{parentName:"h4",id:"fnref-35"},(0,r.kt)("a",{parentName:"sup",href:"#fn-35",className:"footnote-ref"},"35"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"soft-prompting"},"Soft Prompting"),(0,r.kt)("h4",{id:"soft-promptinglester2021power-"},"Soft Prompting",(0,r.kt)("sup",{parentName:"h4",id:"fnref-36"},(0,r.kt)("a",{parentName:"sup",href:"#fn-36",className:"footnote-ref"},"36"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"interpretable-discretized-soft-promptskhashabi2021prompt-"},"Interpretable Discretized Soft Prompts",(0,r.kt)("sup",{parentName:"h4",id:"fnref-37"},(0,r.kt)("a",{parentName:"sup",href:"#fn-37",className:"footnote-ref"},"37"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"datasets"},"Datasets"),(0,r.kt)("h4",{id:"multiarithroy-roth-2015-solving-"},"MultiArith",(0,r.kt)("sup",{parentName:"h4",id:"fnref-38"},(0,r.kt)("a",{parentName:"sup",href:"#fn-38",className:"footnote-ref"},"38"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"gsm8kcobbe2021training-"},"GSM8K",(0,r.kt)("sup",{parentName:"h4",id:"fnref-39"},(0,r.kt)("a",{parentName:"sup",href:"#fn-39",className:"footnote-ref"},"39"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"hotpotqayang2018hotpotqa-"},"HotPotQA",(0,r.kt)("sup",{parentName:"h4",id:"fnref-40"},(0,r.kt)("a",{parentName:"sup",href:"#fn-40",className:"footnote-ref"},"40"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"feverthorne2018fever-"},"Fever",(0,r.kt)("sup",{parentName:"h4",id:"fnref-41"},(0,r.kt)("a",{parentName:"sup",href:"#fn-41",className:"footnote-ref"},"41"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"bbq-a-hand-built-bias-benchmark-for-question-answeringparrish2021bbq-"},"BBQ: A Hand-Built Bias Benchmark for Question Answering",(0,r.kt)("sup",{parentName:"h4",id:"fnref-42"},(0,r.kt)("a",{parentName:"sup",href:"#fn-42",className:"footnote-ref"},"42"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"image-prompt-engineering"},"Image Prompt Engineering"),(0,r.kt)("h4",{id:"taxonomy-of-prompt-modifiersoppenlaender2022taxonomy"},"Taxonomy of prompt modifiers",(0,r.kt)("sup",{parentName:"h4",id:"fnref-43"},(0,r.kt)("a",{parentName:"sup",href:"#fn-43",className:"footnote-ref"},"43"))),(0,r.kt)("h4",{id:"diffusiondbwang2022diffusiondb"},"DiffusionDB",(0,r.kt)("sup",{parentName:"h4",id:"fnref-44"},(0,r.kt)("a",{parentName:"sup",href:"#fn-44",className:"footnote-ref"},"44"))),(0,r.kt)("h4",{id:"the-dalle-2-prompt-bookparsons2022dalleprompt-"},"The DALLE 2 Prompt Book",(0,r.kt)("sup",{parentName:"h4",id:"fnref-45"},(0,r.kt)("a",{parentName:"sup",href:"#fn-45",className:"footnote-ref"},"45"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"prompt-engineering-for-text-based-generative-artoppenlaender2022prompt-"},"Prompt Engineering for Text-Based Generative Art",(0,r.kt)("sup",{parentName:"h4",id:"fnref-46"},(0,r.kt)("a",{parentName:"sup",href:"#fn-46",className:"footnote-ref"},"46"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"with-the-right-prompt-stable-diffusion-20-can-do-handsblake2022with-"},"With the right prompt, Stable Diffusion 2.0 can do hands.",(0,r.kt)("sup",{parentName:"h4",id:"fnref-47"},(0,r.kt)("a",{parentName:"sup",href:"#fn-47",className:"footnote-ref"},"47"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"optimizing-prompts-for-text-to-image-generationhao2022optimizing"},"Optimizing Prompts for Text-to-Image Generation",(0,r.kt)("sup",{parentName:"h4",id:"fnref-48"},(0,r.kt)("a",{parentName:"sup",href:"#fn-48",className:"footnote-ref"},"48"))),(0,r.kt)("h2",{id:"prompt-engineering-ides"},"Prompt Engineering IDEs"),(0,r.kt)("h4",{id:"prompt-idestrobelt2022promptide-"},"Prompt IDE",(0,r.kt)("sup",{parentName:"h4",id:"fnref-49"},(0,r.kt)("a",{parentName:"sup",href:"#fn-49",className:"footnote-ref"},"49"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"prompt-sourcebach2022promptsource-"},"Prompt Source",(0,r.kt)("sup",{parentName:"h4",id:"fnref-50"},(0,r.kt)("a",{parentName:"sup",href:"#fn-50",className:"footnote-ref"},"50"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"promptchainerwu2022promptchainer-"},"PromptChainer",(0,r.kt)("sup",{parentName:"h4",id:"fnref-51"},(0,r.kt)("a",{parentName:"sup",href:"#fn-51",className:"footnote-ref"},"51"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"promptmakerjiang2022promptmaker-"},"PromptMaker",(0,r.kt)("sup",{parentName:"h4",id:"fnref-52"},(0,r.kt)("a",{parentName:"sup",href:"#fn-52",className:"footnote-ref"},"52"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"tooling"},"Tooling"),(0,r.kt)("h4",{id:"langchainchase_langchain_2022-"},"LangChain",(0,r.kt)("sup",{parentName:"h4",id:"fnref-53"},(0,r.kt)("a",{parentName:"sup",href:"#fn-53",className:"footnote-ref"},"53"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"textbox-20-a-text-generation-library-with-pre-trained-language-modelstang2022textbox-"},"TextBox 2.0: A Text Generation Library with Pre-trained Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-54"},(0,r.kt)("a",{parentName:"sup",href:"#fn-54",className:"footnote-ref"},"54"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"openprompt-an-open-source-framework-for-prompt-learningding2021openprompt-"},"OpenPrompt: An Open-source Framework for Prompt-learning",(0,r.kt)("sup",{parentName:"h4",id:"fnref-55"},(0,r.kt)("a",{parentName:"sup",href:"#fn-55",className:"footnote-ref"},"55"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"gpt-indexliu_gpt_index_2022-"},"GPT Index",(0,r.kt)("sup",{parentName:"h4",id:"fnref-56"},(0,r.kt)("a",{parentName:"sup",href:"#fn-56",className:"footnote-ref"},"56"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"applied-prompt-engineering"},"Applied Prompt Engineering"),(0,r.kt)("h4",{id:"language-model-cascadesdohan2022language"},"Language Model Cascades",(0,r.kt)("sup",{parentName:"h4",id:"fnref-57"},(0,r.kt)("a",{parentName:"sup",href:"#fn-57",className:"footnote-ref"},"57"))),(0,r.kt)("h4",{id:"mrklkarpas2022mrkl-"},"MRKL",(0,r.kt)("sup",{parentName:"h4",id:"fnref-58"},(0,r.kt)("a",{parentName:"sup",href:"#fn-58",className:"footnote-ref"},"58"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"reactyao2022react-"},"ReAct",(0,r.kt)("sup",{parentName:"h4",id:"fnref-59"},(0,r.kt)("a",{parentName:"sup",href:"#fn-59",className:"footnote-ref"},"59"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"pal-program-aided-language-modelsgao2022pal-"},"PAL: Program-aided Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-60"},(0,r.kt)("a",{parentName:"sup",href:"#fn-60",className:"footnote-ref"},"60"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"user-interface-design"},"User Interface Design"),(0,r.kt)("h4",{id:"design-guidelines-for-prompt-engineering-text-to-image-generative-modelsliu2022design"},"Design Guidelines for Prompt Engineering Text-to-Image Generative Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-61"},(0,r.kt)("a",{parentName:"sup",href:"#fn-61",className:"footnote-ref"},"61"))),(0,r.kt)("h2",{id:"prompt-injection"},"Prompt Injection"),(0,r.kt)("h4",{id:"machine-generated-text-a-comprehensive-survey-of-threat-models-and-detection-methodscrothers2022machine-"},"Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods",(0,r.kt)("sup",{parentName:"h4",id:"fnref-62"},(0,r.kt)("a",{parentName:"sup",href:"#fn-62",className:"footnote-ref"},"62"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"evaluating-the-susceptibility-of-pre-trained-language-models-via-handcrafted-adversarial-examplesbranch2022evaluating-"},"Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples",(0,r.kt)("sup",{parentName:"h4",id:"fnref-63"},(0,r.kt)("a",{parentName:"sup",href:"#fn-63",className:"footnote-ref"},"63"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"prompt-injection-attacks-against-gpt-3simon2022inject-"},"Prompt injection attacks against GPT-3",(0,r.kt)("sup",{parentName:"h4",id:"fnref-64"},(0,r.kt)("a",{parentName:"sup",href:"#fn-64",className:"footnote-ref"},"64"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"exploiting-gpt-3-prompts-with-malicious-inputs-that-order-the-model-to-ignore-its-previous-directionsgoodside2022inject-"},"Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions",(0,r.kt)("sup",{parentName:"h4",id:"fnref-65"},(0,r.kt)("a",{parentName:"sup",href:"#fn-65",className:"footnote-ref"},"65"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"adversarial-promptschase2021adversarial-"},"adversarial-prompts",(0,r.kt)("sup",{parentName:"h4",id:"fnref-66"},(0,r.kt)("a",{parentName:"sup",href:"#fn-66",className:"footnote-ref"},"66"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"gpt-3-prompt-injection-defensesgoodside2021gpt-"},"GPT-3 Prompt Injection Defenses",(0,r.kt)("sup",{parentName:"h4",id:"fnref-67"},(0,r.kt)("a",{parentName:"sup",href:"#fn-67",className:"footnote-ref"},"67"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"talking-to-machines-prompt-engineering--injectionchristoph2022talking"},"Talking to machines: prompt engineering & injection",(0,r.kt)("sup",{parentName:"h4",id:"fnref-68"},(0,r.kt)("a",{parentName:"sup",href:"#fn-68",className:"footnote-ref"},"68"))),(0,r.kt)("h4",{id:"exploring-prompt-injection-attacksselvi2022exploring-"},"Exploring Prompt Injection Attacks",(0,r.kt)("sup",{parentName:"h4",id:"fnref-69"},(0,r.kt)("a",{parentName:"sup",href:"#fn-69",className:"footnote-ref"},"69"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"using-gpt-eliezer-against-chatgpt-jailbreakingarmstrong2022using-"},"Using GPT-Eliezer against ChatGPT Jailbreaking",(0,r.kt)("sup",{parentName:"h4",id:"fnref-70"},(0,r.kt)("a",{parentName:"sup",href:"#fn-70",className:"footnote-ref"},"70"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"microsoft-bing-chat-promptkevinbing"},"Microsoft Bing Chat Prompt",(0,r.kt)("sup",{parentName:"h4",id:"fnref-71"},(0,r.kt)("a",{parentName:"sup",href:"#fn-71",className:"footnote-ref"},"71"))),(0,r.kt)("h2",{id:"jailbreaking"},"Jailbreaking"),(0,r.kt)("h4",{id:"ignore-previous-prompt-attack-techniques-for-language-modelsperez2022jailbreak"},"Ignore Previous Prompt: Attack Techniques For Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-72"},(0,r.kt)("a",{parentName:"sup",href:"#fn-72",className:"footnote-ref"},"72"))),(0,r.kt)("h4",{id:"lessons-learned-on-language-model-safety-and-misusebrundage_2022"},"Lessons learned on Language Model Safety and misuse",(0,r.kt)("sup",{parentName:"h4",id:"fnref-73"},(0,r.kt)("a",{parentName:"sup",href:"#fn-73",className:"footnote-ref"},"73"))),(0,r.kt)("h4",{id:"toxicity-detection-with-generative-prompt-based-inferencewang2022jailbreak"},"Toxicity Detection with Generative Prompt-based Inference",(0,r.kt)("sup",{parentName:"h4",id:"fnref-74"},(0,r.kt)("a",{parentName:"sup",href:"#fn-74",className:"footnote-ref"},"74"))),(0,r.kt)("h4",{id:"new-and-improved-content-moderation-toolingmarkov_2022"},"New and improved content moderation tooling",(0,r.kt)("sup",{parentName:"h4",id:"fnref-75"},(0,r.kt)("a",{parentName:"sup",href:"#fn-75",className:"footnote-ref"},"75"))),(0,r.kt)("h4",{id:"openai-apiopenai_api-"},"OpenAI API",(0,r.kt)("sup",{parentName:"h4",id:"fnref-76"},(0,r.kt)("a",{parentName:"sup",href:"#fn-76",className:"footnote-ref"},"76"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"openai-chatgptopenai_chatgpt-"},"OpenAI ChatGPT",(0,r.kt)("sup",{parentName:"h4",id:"fnref-77"},(0,r.kt)("a",{parentName:"sup",href:"#fn-77",className:"footnote-ref"},"77"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"chatgpt-4-tweetalice2022jailbreak-"},"ChatGPT 4 Tweet",(0,r.kt)("sup",{parentName:"h4",id:"fnref-78"},(0,r.kt)("a",{parentName:"sup",href:"#fn-78",className:"footnote-ref"},"78"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"acting-tweetmiguel2022jailbreak-"},"Acting Tweet",(0,r.kt)("sup",{parentName:"h4",id:"fnref-79"},(0,r.kt)("a",{parentName:"sup",href:"#fn-79",className:"footnote-ref"},"79"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"research-tweetderek2022jailbreak-"},"Research Tweet",(0,r.kt)("sup",{parentName:"h4",id:"fnref-80"},(0,r.kt)("a",{parentName:"sup",href:"#fn-80",className:"footnote-ref"},"80"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"pretend-ability-tweetnero2022jailbreak-"},"Pretend Ability Tweet",(0,r.kt)("sup",{parentName:"h4",id:"fnref-81"},(0,r.kt)("a",{parentName:"sup",href:"#fn-81",className:"footnote-ref"},"81"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"responsibility-tweetnick2022jailbreak-"},"Responsibility Tweet",(0,r.kt)("sup",{parentName:"h4",id:"fnref-82"},(0,r.kt)("a",{parentName:"sup",href:"#fn-82",className:"footnote-ref"},"82"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"lynx-mode-tweetjonas2022jailbreak-"},"Lynx Mode Tweet",(0,r.kt)("sup",{parentName:"h4",id:"fnref-83"},(0,r.kt)("a",{parentName:"sup",href:"#fn-83",className:"footnote-ref"},"83"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"sudo-mode-tweetsudo2022jailbreak-"},"Sudo Mode Tweet",(0,r.kt)("sup",{parentName:"h4",id:"fnref-84"},(0,r.kt)("a",{parentName:"sup",href:"#fn-84",className:"footnote-ref"},"84"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"ignore-previous-promptignore_previous_prompt-"},"Ignore Previous Prompt",(0,r.kt)("sup",{parentName:"h4",id:"fnref-85"},(0,r.kt)("a",{parentName:"sup",href:"#fn-85",className:"footnote-ref"},"85"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"updated-jailbreaking-prompts-ai_jailbreak-"},"Updated Jailbreaking Prompts",(0,r.kt)("sup",{parentName:"h4",id:"fnref-86"},(0,r.kt)("a",{parentName:"sup",href:"#fn-86",className:"footnote-ref"},"86"))," \ud83d\udd35"),(0,r.kt)("h2",{id:"surveys"},"Surveys"),(0,r.kt)("h4",{id:"pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processingliu2021pretrain"},"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",(0,r.kt)("sup",{parentName:"h4",id:"fnref-87"},(0,r.kt)("a",{parentName:"sup",href:"#fn-87",className:"footnote-ref"},"87"))),(0,r.kt)("h4",{id:"promptpapersning2022papers"},"PromptPapers",(0,r.kt)("sup",{parentName:"h4",id:"fnref-88"},(0,r.kt)("a",{parentName:"sup",href:"#fn-88",className:"footnote-ref"},"88"))),(0,r.kt)("h2",{id:"dataset-generation"},"Dataset Generation"),(0,r.kt)("h4",{id:"discovering-language-model-behaviors-with-model-written-evaluationsperez2022discovering"},"Discovering Language Model Behaviors with Model-Written Evaluations",(0,r.kt)("sup",{parentName:"h4",id:"fnref-89"},(0,r.kt)("a",{parentName:"sup",href:"#fn-89",className:"footnote-ref"},"89"))),(0,r.kt)("h4",{id:"selective-annotation-makes-language-models-better-few-shot-learnerssu2022selective"},"Selective Annotation Makes Language Models Better Few-Shot Learners",(0,r.kt)("sup",{parentName:"h4",id:"fnref-90"},(0,r.kt)("a",{parentName:"sup",href:"#fn-90",className:"footnote-ref"},"90"))),(0,r.kt)("h2",{id:"applications"},"Applications"),(0,r.kt)("h4",{id:"atlas-few-shot-learning-with-retrieval-augmented-language-modelsizacard2022atlas"},"Atlas: Few-shot Learning with Retrieval Augmented Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-91"},(0,r.kt)("a",{parentName:"sup",href:"#fn-91",className:"footnote-ref"},"91"))),(0,r.kt)("h4",{id:"strudel-structured-dialogue-summarization-for-dialogue-comprehensionwang2022strudel"},"STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension",(0,r.kt)("sup",{parentName:"h4",id:"fnref-92"},(0,r.kt)("a",{parentName:"sup",href:"#fn-92",className:"footnote-ref"},"92"))),(0,r.kt)("h2",{id:"miscl"},"Miscl"),(0,r.kt)("h4",{id:"prompting-is-programming-a-query-language-for-large-language-modelsbeurerkellner2022prompting"},"Prompting Is Programming: A Query Language For Large Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-93"},(0,r.kt)("a",{parentName:"sup",href:"#fn-93",className:"footnote-ref"},"93"))),(0,r.kt)("h4",{id:"parallel-context-windows-improve-in-context-learning-of-large-language-modelsratner2022parallel"},"Parallel Context Windows Improve In-Context Learning of Large Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-94"},(0,r.kt)("a",{parentName:"sup",href:"#fn-94",className:"footnote-ref"},"94"))),(0,r.kt)("h4",{id:"a-prompt-pattern-catalog-to-enhance-prompt-engineering-with-chatgptwhite2023prompt-"},"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT",(0,r.kt)("sup",{parentName:"h4",id:"fnref-95"},(0,r.kt)("a",{parentName:"sup",href:"#fn-95",className:"footnote-ref"},"95"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"learning-to-perform-complex-tasks-through-compositional-fine-tuning-of-language-modelsbursztyn2022learning"},"Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-96"},(0,r.kt)("a",{parentName:"sup",href:"#fn-96",className:"footnote-ref"},"96"))),(0,r.kt)("h4",{id:"super-naturalinstructions-generalization-via-declarative-instructions-on-1600-nlp-taskswang2022supernaturalinstructions"},"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",(0,r.kt)("sup",{parentName:"h4",id:"fnref-97"},(0,r.kt)("a",{parentName:"sup",href:"#fn-97",className:"footnote-ref"},"97"))),(0,r.kt)("h4",{id:"making-pre-trained-language-models-better-few-shot-learnersgao2021making"},"Making Pre-trained Language Models Better Few-shot Learners",(0,r.kt)("sup",{parentName:"h4",id:"fnref-98"},(0,r.kt)("a",{parentName:"sup",href:"#fn-98",className:"footnote-ref"},"98"))),(0,r.kt)("h4",{id:"grounding-with-search-resultslivin2022large"},"Grounding with search results",(0,r.kt)("sup",{parentName:"h4",id:"fnref-99"},(0,r.kt)("a",{parentName:"sup",href:"#fn-99",className:"footnote-ref"},"99"))),(0,r.kt)("h4",{id:"how-to-prompt-opportunities-and-challenges-of-zero--and-few-shot-learning-for-human-ai-interaction-in-creative-applications-of-generative-modelsdang2022prompt"},"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-100"},(0,r.kt)("a",{parentName:"sup",href:"#fn-100",className:"footnote-ref"},"100"))),(0,r.kt)("h4",{id:"on-measuring-social-biases-in-prompt-based-multi-task-learningakyrek2022measuring"},"On Measuring Social Biases in Prompt-Based Multi-Task Learning",(0,r.kt)("sup",{parentName:"h4",id:"fnref-101"},(0,r.kt)("a",{parentName:"sup",href:"#fn-101",className:"footnote-ref"},"101"))),(0,r.kt)("h4",{id:"plot-writing-from-pre-trained-language-modelsjin2022plot-"},"Plot Writing From Pre-Trained Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-102"},(0,r.kt)("a",{parentName:"sup",href:"#fn-102",className:"footnote-ref"},"102"))," \ud83d\udd35"),(0,r.kt)("h4",{id:"stereoset-measuring-stereotypical-bias-in-pretrained-language-modelsnadeem-etal-2021-stereoset"},"StereoSet: Measuring stereotypical bias in pretrained language models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-103"},(0,r.kt)("a",{parentName:"sup",href:"#fn-103",className:"footnote-ref"},"103"))),(0,r.kt)("h4",{id:"survey-of-hallucination-in-natural-language-generationji_2022"},"Survey of Hallucination in Natural Language Generation",(0,r.kt)("sup",{parentName:"h4",id:"fnref-104"},(0,r.kt)("a",{parentName:"sup",href:"#fn-104",className:"footnote-ref"},"104"))),(0,r.kt)("h4",{id:"examples2022examples"},"Examples",(0,r.kt)("sup",{parentName:"h4",id:"fnref-105"},(0,r.kt)("a",{parentName:"sup",href:"#fn-105",className:"footnote-ref"},"105"))),(0,r.kt)("h4",{id:"wordcraftyuan2022wordcraft"},"Wordcraft",(0,r.kt)("sup",{parentName:"h4",id:"fnref-106"},(0,r.kt)("a",{parentName:"sup",href:"#fn-106",className:"footnote-ref"},"106"))),(0,r.kt)("h4",{id:"painpointsfadnavis2022pain"},"PainPoints",(0,r.kt)("sup",{parentName:"h4",id:"fnref-107"},(0,r.kt)("a",{parentName:"sup",href:"#fn-107",className:"footnote-ref"},"107"))),(0,r.kt)("h4",{id:"self-instruct-aligning-language-model-with-self-generated-instructionswang2022selfinstruct"},"Self-Instruct: Aligning Language Model with Self Generated Instructions",(0,r.kt)("sup",{parentName:"h4",id:"fnref-108"},(0,r.kt)("a",{parentName:"sup",href:"#fn-108",className:"footnote-ref"},"108"))),(0,r.kt)("h4",{id:"from-images-to-textual-prompts-zero-shot-vqa-with-frozen-large-language-modelsguo2022images"},"From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models",(0,r.kt)("sup",{parentName:"h4",id:"fnref-109"},(0,r.kt)("a",{parentName:"sup",href:"#fn-109",className:"footnote-ref"},"109"))),(0,r.kt)("h4",{id:"exploiting-cloze-questions-for-few-shot-text-classification-and-natural-language-inferenceschick2020exploiting"},"Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference",(0,r.kt)("sup",{parentName:"h4",id:"fnref-110"},(0,r.kt)("a",{parentName:"sup",href:"#fn-110",className:"footnote-ref"},"110"))),(0,r.kt)("h3",{id:"ask-me-anything-promptingarora2022ama"},"Ask-Me-Anything Prompting",(0,r.kt)("sup",{parentName:"h3",id:"fnref-5"},(0,r.kt)("a",{parentName:"sup",href:"#fn-5",className:"footnote-ref"},"5"))),(0,r.kt)("h3",{id:"a-watermark-for-large-language-modelskirchenbauer2023watermarking"},"A Watermark for Large Language Models",(0,r.kt)("sup",{parentName:"h3",id:"fnref-111"},(0,r.kt)("a",{parentName:"sup",href:"#fn-111",className:"footnote-ref"},"111"))),(0,r.kt)("div",{className:"footnotes"},(0,r.kt)("hr",{parentName:"div"}),(0,r.kt)("ol",{parentName:"div"},(0,r.kt)("li",{parentName:"ol",id:"fn-1"},"Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-1",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-2"},"Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-2",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-3"},"Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., & Zhou, D. (2022). Self-Consistency Improves Chain of Thought Reasoning in Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-3",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-4"},"Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., & Chen, W. (2021). What Makes Good In-Context Examples for GPT-3?\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-4",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-5"},"Arora, S., Narayan, A., Chen, M. F., Orr, L., Guha, N., Bhatia, K., Chami, I., Sala, F., & R\xe9, C. (2022). Ask Me Anything: A simple strategy for prompting language models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-5",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-6"},"Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Bras, R. L., Choi, Y., & Hajishirzi, H. (2021). Generated Knowledge Prompting for Commonsense Reasoning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-6",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-7"},"Sun, Z., Wang, X., Tay, Y., Yang, Y., & Zhou, D. (2022). Recitation-Augmented Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-7",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-8"},"Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., & Zettlemoyer, L. (2022). Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-8",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-9"},"Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., & Odena, A. (2021). Show Your Work: Scratchpads for Intermediate Computation with Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-9",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-10"},"Jung, J., Qin, L., Welleck, S., Brahman, F., Bhagavatula, C., Bras, R. L., & Choi, Y. (2022). Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-10",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-11"},"Zelikman, E., Wu, Y., Mu, J., & Goodman, N. D. (2022). STaR: Bootstrapping Reasoning With Reasoning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-11",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-12"},"Zhou, D., Sch\xe4rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., & Chi, E. (2022). Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-12",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-13"},"Mishra, S., Khashabi, D., Baral, C., Choi, Y., & Hajishirzi, H. (2022). Reframing Instructional Prompts to GPTk\u2019s Language. Findings of the Association for Computational Linguistics: ACL 2022. https://doi.org/10.18653/v1/2022.findings-acl.50\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-13",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-14"},"Efrat, A., & Levy, O. (2020). The Turking Test: Can Language Models Understand Instructions?\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-14",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-15"},"Imani, S., Du, L., & Shrivastava, H. (2023). MathPrompter: Mathematical Reasoning using Large Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-15",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-16"},"Ye, X., & Durrett, G. (2022). The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-16",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-17"},"Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., & Wang, L. (2022). Prompting GPT-3 To Be Reliable.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-17",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-18"},"Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., & Chen, W. (2022). On the Advance of Making Language Models Better Reasoners.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-18",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-19"},"Zhao, T. Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021). Calibrate Before Use: Improving Few-Shot Performance of Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-19",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-20"},"Mitchell, E., Noh, J. J., Li, S., Armstrong, W. S., Agarwal, A., Liu, P., Finn, C., & Manning, C. D. (2022). Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-20",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-21"},"Shaikh, O., Zhang, H., Held, W., Bernstein, M., & Yang, D. (2022). On Second Thought, Let\u2019s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-21",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-22"},"Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah, C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Perez, E., \u2026 Kaplan, J. (2022). Constitutional AI: Harmlessness from AI Feedback.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-22",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-23"},"Lake, B. M., & Baroni, M. (2018). Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks. https://doi.org/10.48550/arXiv.1711.00350\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-23",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-24"},"Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., & Singh, S. (2020). AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). https://doi.org/10.18653/v1/2020.emnlp-main.346\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-24",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-25"},"Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., & Ba, J. (2022). Large Language Models Are Human-Level Prompt Engineers.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-25",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-26"},"Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., \u2026 Amodei, D. (2020). Language Models are Few-Shot Learners.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-26",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-27"},"Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). Training language models to follow instructions with human feedback.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-27",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-28"},"Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., \u2026 Fiedel, N. (2022). PaLM: Scaling Language Modeling with Pathways.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-28",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-29"},"Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u0107, S., Hesslow, D., Castagn\xe9, R., Luccioni, A. S., Yvon, F., Gall\xe9, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., \u2026 Wolf, T. (2022). BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-29",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-30"},"Yong, Z.-X., Schoelkopf, H., Muennighoff, N., Aji, A. F., Adelani, D. I., Almubarak, K., Bari, M. S., Sutawika, L., Kasai, J., Baruwa, A., Winata, G. I., Biderman, S., Radev, D., & Nikoulina, V. (2022). BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-30",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-31"},"Lieber, O., Sharir, O., Lentz, B., & Shoham, Y. (2021). Jurassic-1: Technical Details and Evaluation, White paper, AI21 Labs, 2021. URL: Https://Uploads-Ssl. Webflow. Com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_ Tech_paper. Pdf.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-31",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-32"},"Wang, B., & Komatsuzaki, A. (2021). GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax. https://github.com/kingoflolz/mesh-transformer-jax\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-32",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-33"},"Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv Preprint arXiv:1907.11692.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-33",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-34"},"Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). High-Resolution Image Synthesis with Latent Diffusion Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-34",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-35"},"Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical Text-Conditional Image Generation with CLIP Latents.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-35",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-36"},"Lester, B., Al-Rfou, R., & Constant, N. (2021). The Power of Scale for Parameter-Efficient Prompt Tuning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-36",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-37"},"Khashabi, D., Lyu, S., Min, S., Qin, L., Richardson, K., Welleck, S., Hajishirzi, H., Khot, T., Sabharwal, A., Singh, S., & Choi, Y. (2021). Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-37",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-38"},"Roy, S., & Roth, D. (2015). Solving General Arithmetic Word Problems. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 1743\u20131752. https://doi.org/10.18653/v1/D15-1202\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-38",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-39"},"Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., & Schulman, J. (2021). Training Verifiers to Solve Math Word Problems.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-39",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-40"},"Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., & Manning, C. D. (2018). HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-40",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-41"},"Thorne, J., Vlachos, A., Christodoulopoulos, C., & Mittal, A. (2018). FEVER: a large-scale dataset for Fact Extraction and VERification.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-41",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-42"},"Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., Htut, P. M., & Bowman, S. R. (2021). BBQ: A Hand-Built Bias Benchmark for Question Answering.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-42",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-43"},"Oppenlaender, J. (2022). A Taxonomy of Prompt Modifiers for Text-To-Image Generation.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-43",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-44"},"Wang, Z. J., Montoya, E., Munechika, D., Yang, H., Hoover, B., & Chau, D. H. (2022). DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-44",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-45"},"Parsons, G. (2022). The DALLE 2 Prompt Book. https://dallery.gallery/the-dalle-2-prompt-book/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-45",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-46"},"Oppenlaender, J. (2022). Prompt Engineering for Text-Based Generative Art.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-46",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-47"},"Blake. (2022). With the right prompt, Stable Diffusion 2.0 can do hands. https://www.reddit.com/r/StableDiffusion/comments/z7salo/with_the_right_prompt_stable_diffusion_20_can_do/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-47",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-48"},"Hao, Y., Chi, Z., Dong, L., & Wei, F. (2022). Optimizing Prompts for Text-to-Image Generation.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-48",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-49"},"Strobelt, H., Webson, A., Sanh, V., Hoover, B., Beyer, J., Pfister, H., & Rush, A. M. (2022). Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models. arXiv. https://doi.org/10.48550/ARXIV.2208.07852\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-49",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-50"},"Bach, S. H., Sanh, V., Yong, Z.-X., Webson, A., Raffel, C., Nayak, N. V., Sharma, A., Kim, T., Bari, M. S., Fevry, T., Alyafeai, Z., Dey, M., Santilli, A., Sun, Z., Ben-David, S., Xu, C., Chhablani, G., Wang, H., Fries, J. A., \u2026 Rush, A. M. (2022). PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-50",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-51"},"Wu, T., Jiang, E., Donsbach, A., Gray, J., Molina, A., Terry, M., & Cai, C. J. (2022). PromptChainer: Chaining Large Language Model Prompts through Visual Programming.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-51",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-52"},"Jiang, E., Olson, K., Toh, E., Molina, A., Donsbach, A., Terry, M., & Cai, C. J. (2022). PromptMaker: Prompt-Based Prototyping with Large&nbsp;Language&nbsp;Models. Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3491101.3503564\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-52",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-53"},"Chase, H. (2022). LangChain (0.0.66) [Computer software]. https://github.com/hwchase17/langchain\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-53",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-54"},"Tang, T., Junyi, L., Chen, Z., Hu, Y., Yu, Z., Dai, W., Dong, Z., Cheng, X., Wang, Y., Zhao, W., Nie, J., & Wen, J.-R. (2022). TextBox 2.0: A Text Generation Library with Pre-trained Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-54",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-55"},"Ding, N., Hu, S., Zhao, W., Chen, Y., Liu, Z., Zheng, H.-T., & Sun, M. (2021). OpenPrompt: An Open-source Framework for Prompt-learning. arXiv Preprint arXiv:2111.01998.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-55",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-56"},"Liu, J. (2022). GPT Index. https://doi.org/10.5281/zenodo.1234\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-56",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-57"},"Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-dickstein, J., Murphy, K., & Sutton, C. (2022). Language Model Cascades.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-57",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-58"},"Karpas, E., Abend, O., Belinkov, Y., Lenz, B., Lieber, O., Ratner, N., Shoham, Y., Bata, H., Levine, Y., Leyton-Brown, K., Muhlgay, D., Rozen, N., Schwartz, E., Shachaf, G., Shalev-Shwartz, S., Shashua, A., & Tenenholtz, M. (2022). MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-58",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-59"},"Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2022). ReAct: Synergizing Reasoning and Acting in Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-59",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-60"},"Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., & Neubig, G. (2022). PAL: Program-aided Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-60",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-61"},"Liu, V., & Chilton, L. B. (2022). Design Guidelines for Prompt Engineering Text-to-Image Generative Models. Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3491102.3501825\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-61",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-62"},"Crothers, E., Japkowicz, N., & Viktor, H. (2022). Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-62",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-63"},"Branch, H. J., Cefalu, J. R., McHugh, J., Hujer, L., Bahl, A., del Castillo Iglesias, D., Heichman, R., & Darwishi, R. (2022). Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-63",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-64"},"Willison, S. (2022). Prompt injection attacks against GPT-3. https://simonwillison.net/2022/Sep/12/prompt-injection/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-64",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-65"},"Goodside, R. (2022). Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions. https://twitter.com/goodside/status/1569128808308957185\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-65",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-66"},"Chase, H. (2022). adversarial-prompts. https://github.com/hwchase17/adversarial-prompts\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-66",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-67"},"Goodside, R. (2022). GPT-3 Prompt Injection Defenses. https://twitter.com/goodside/status/1578278974526222336?s=20&t=3UMZB7ntYhwAk3QLpKMAbw\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-67",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-68"},"Mark, C. (2022). Talking to machines: prompt engineering & injection. https://artifact-research.com/artificial-intelligence/talking-to-machines-prompt-engineering-injection/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-68",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-69"},"Selvi, J. (2022). Exploring Prompt Injection Attacks. https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-69",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-70"},"Stuart Armstrong, R. G. (2022). Using GPT-Eliezer against ChatGPT Jailbreaking. https://www.alignmentforum.org/posts/pNcFYZnPdXyL2RfgA/using-gpt-eliezer-against-chatgpt-jailbreaking\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-70",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-71"},"The entire prompt of Microsoft Bing Chat?! (Hi, Sydney.). (2023). https://twitter.com/kliu128/status/1623472922374574080\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-71",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-72"},"Perez, F., & Ribeiro, I. (2022). Ignore Previous Prompt: Attack Techniques For Language Models. arXiv. https://doi.org/10.48550/ARXIV.2211.09527\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-72",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-73"},"Brundage, M. (2022). Lessons learned on Language Model Safety and misuse. In OpenAI. OpenAI. https://openai.com/blog/language-model-safety-and-misuse/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-73",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-74"},"Wang, Y.-S., & Chang, Y. (2022). Toxicity Detection with Generative Prompt-based Inference. arXiv. https://doi.org/10.48550/ARXIV.2205.12390\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-74",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-75"},"Markov, T. (2022). New and improved content moderation tooling. In OpenAI. OpenAI. https://openai.com/blog/new-and-improved-content-moderation-tooling/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-75",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-76"},"(2022). https://beta.openai.com/docs/guides/moderation\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-76",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-77"},"(2022). https://openai.com/blog/chatgpt/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-77",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-78"},"ok I saw a few people jailbreaking safeguards openai put on chatgpt so I had to give it a shot myself. (2022). https://twitter.com/alicemazzy/status/1598288519301976064\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-78",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-79"},"Bypass @OpenAI\u2019s ChatGPT alignment efforts with this one weird trick. (2022). https://twitter.com/m1guelpf/status/1598203861294252033\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-79",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-80"},"ChatGPT jailbreaking itself. (2022). https://twitter.com/haus_cole/status/1598541468058390534\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-80",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-81"},"Using \u201cpretend\u201d on #ChatGPT can do some wild stuff. You can kind of get some insight on the future, alternative universe. (2022). https://twitter.com/NeroSoares/status/1608527467265904643\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-81",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-82"},"I kinda like this one even more! (2022). https://twitter.com/NickEMoran/status/1598101579626057728\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-82",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-83"},"Degrave, J. (2022). Building A Virtual Machine inside ChatGPT. Engraved. https://www.engraved.blog/building-a-virtual-machine-inside/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-83",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-84"},"(2022). https://www.sudo.ws/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-84",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-85"},"Perez, F., & Ribeiro, I. (2022). Ignore Previous Prompt: Attack Techniques For Language Models. arXiv. https://doi.org/10.48550/ARXIV.2211.09527\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-85",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-86"},"AIWithVibes. (2023). 7 ChatGPT JailBreaks and Content Filters Bypass that work. https://chatgpt-jailbreak.super.site/\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-86",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-87"},"Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2022). Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. ACM Computing Surveys. https://doi.org/10.1145/3560815\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-87",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-88"},"PromptPapers. (2022). https://github.com/thunlp/PromptPapers\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-88",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-89"},"Perez, E., Ringer, S., Luko\u0161i\u016bt\u0117, K., Nguyen, K., Chen, E., Heiner, S., Pettit, C., Olsson, C., Kundu, S., Kadavath, S., Jones, A., Chen, A., Mann, B., Israel, B., Seethor, B., McKinnon, C., Olah, C., Yan, D., Amodei, D., \u2026 Kaplan, J. (2022). Discovering Language Model Behaviors with Model-Written Evaluations.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-89",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-90"},"Su, H., Kasai, J., Wu, C. H., Shi, W., Wang, T., Xin, J., Zhang, R., Ostendorf, M., Zettlemoyer, L., Smith, N. A., & Yu, T. (2022). Selective Annotation Makes Language Models Better Few-Shot Learners.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-90",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-91"},"Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., & Grave, E. (2022). Atlas: Few-shot Learning with Retrieval Augmented Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-91",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-92"},"Wang, B., Feng, C., Nair, A., Mao, M., Desai, J., Celikyilmaz, A., Li, H., Mehdad, Y., & Radev, D. (2022). STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-92",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-93"},"Beurer-Kellner, L., Fischer, M., & Vechev, M. (2022). Prompting Is Programming: A Query Language For Large Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-93",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-94"},"Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Abend, O., Karpas, E., Shashua, A., Leyton-Brown, K., & Shoham, Y. (2022). Parallel Context Windows Improve In-Context Learning of Large Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-94",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-95"},"White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, J., & Schmidt, D. C. (2023). A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-95",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-96"},"Bursztyn, V. S., Demeter, D., Downey, D., & Birnbaum, L. (2022). Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-96",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-97"},"Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., Pathak, E., Karamanolakis, G., Lai, H. G., Purohit, I., Mondal, I., Anderson, J., Kuznia, K., Doshi, K., Patel, M., \u2026 Khashabi, D. (2022). Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-97",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-98"},"Gao, T., Fisch, A., & Chen, D. (2021). Making Pre-trained Language Models Better Few-shot Learners. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). https://doi.org/10.18653/v1/2021.acl-long.295\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-98",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-99"},"Li\xe9vin, V., Hother, C. E., & Winther, O. (2022). Can large language models reason about medical questions?\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-99",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-100"},"Dang, H., Mecke, L., Lehmann, F., Goller, S., & Buschek, D. (2022). How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-100",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-101"},"Aky\xfcrek, A. F., Paik, S., Kocyigit, M. Y., Akbiyik, S., Runyun, \u015e. L., & Wijaya, D. (2022). On Measuring Social Biases in Prompt-Based Multi-Task Learning.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-101",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-102"},"Jin, Y., Kadam, V., & Wanvarie, D. (2022). Plot Writing From Pre-Trained Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-102",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-103"},"Nadeem, M., Bethke, A., & Reddy, S. (2021). StereoSet: Measuring stereotypical bias in pretrained language models. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 5356\u20135371. https://doi.org/10.18653/v1/2021.acl-long.416\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-103",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-104"},"Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y., Madotto, A., & Fung, P. (2022). Survey of Hallucination in Natural Language Generation. ACM Computing Surveys. https://doi.org/10.1145/3571730\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-104",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-105"},"Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., & Chen, W. (2022). What Makes Good In-Context Examples for GPT-3? Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. https://doi.org/10.18653/v1/2022.deelio-1.10\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-105",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-106"},"Yuan, A., Coenen, A., Reif, E., & Ippolito, D. (2022). Wordcraft: Story Writing With Large Language Models. 27th International Conference on Intelligent User Interfaces, 841\u2013852.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-106",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-107"},"Fadnavis, S., Dhurandhar, A., Norel, R., Reinen, J. M., Agurto, C., Secchettin, E., Schweiger, V., Perini, G., & Cecchi, G. (2022). PainPoints: A Framework for Language-based Detection of Chronic Pain and Expert-Collaborative Text-Summarization. arXiv Preprint arXiv:2209.09814.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-107",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-108"},"Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., & Hajishirzi, H. (2022). Self-Instruct: Aligning Language Model with Self Generated Instructions.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-108",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-109"},"Guo, J., Li, J., Li, D., Tiong, A. M. H., Li, B., Tao, D., & Hoi, S. C. H. (2022). From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-109",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-110"},"Schick, T., & Sch\xfctze, H. (2020). Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference.\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-110",className:"footnote-backref"},"\u21a9")),(0,r.kt)("li",{parentName:"ol",id:"fn-111"},"Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., & Goldstein, T. (2023). A Watermark for Large Language Models. https://arxiv.org/abs/2301.10226\n",(0,r.kt)("a",{parentName:"li",href:"#fnref-111",className:"footnote-backref"},"\u21a9")))))}g.isMDXComponent=!0}}]);